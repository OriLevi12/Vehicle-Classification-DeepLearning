
"""Deep_Learning_Project_Vehicle_Classification.ipynb

Automatically generated by Colab.

"""

# Imports

import os
import zipfile
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import drive
from torchvision import datasets, transforms, models
from torchvision.models import resnet50, ResNet50_Weights  # NEW - Importing ResNet50 properly
from torch.utils.data import DataLoader, random_split
from sklearn.metrics import confusion_matrix, classification_report
from collections import Counter
from PIL import Image
import random
import shutil

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Usefull Functions and Classes

# EarlyStopping class
class EarlyStopping:
    def __init__(self, patience=5):
        self.patience = patience
        self.counter = 0
        self.best_acc = 0.0
        self.early_stop = False

    def __call__(self, val_acc):
        if val_acc > self.best_acc:
            self.best_acc = val_acc
            self.counter = 0
        else:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True

# Function that checks for corrupted images
def is_valid_image(file_path):
    try:
        img = Image.open(file_path)
        img.verify()
        return True
    except (IOError, SyntaxError):
        return False

# Function that denormalizes images before displaying
def denormalize_image(tensor, mean, std):
    mean = torch.tensor(mean).view(3, 1, 1)
    std = torch.tensor(std).view(3, 1, 1)
    tensor = tensor.clone().detach() * std + mean
    tensor = torch.clamp(tensor, 0, 1)
    return tensor

# Function to show sample images
def show_images(dataset, num_images=5):
    fig, axes = plt.subplots(1, num_images, figsize=(15, 5))
    random_indices = random.sample(range(len(dataset)), num_images)
    for i, idx in enumerate(random_indices):
        img, label = dataset[idx]
        img = denormalize_image(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        img = img.permute(1, 2, 0)
        axes[i].imshow(img.numpy())
        axes[i].set_title(f"Label: {dataset.classes[label]}")
        axes[i].axis('off')
    plt.show()

# Dataset Loading and Management
drive.mount('/content/drive')

zip_file_path = '/content/drive/My Drive/data/vehicles_dataset.zip'
extraction_path = '/content/extracted_files'
data_dir = os.path.join(extraction_path, 'Vehicle Type Image Dataset (Version 2) VTID2')
clean_data_dir = os.path.join(extraction_path, 'clean_vehicle_dataset')

if not os.path.exists(data_dir):
    print("Extracting dataset...")
    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
        zip_ref.extractall(extraction_path)
    print("Extraction complete.")
else:
    print("Dataset already extracted. Skipping extraction.")

# Data Validating and Cleaning

if os.path.exists(clean_data_dir):
    shutil.rmtree(clean_data_dir)
os.makedirs(clean_data_dir)

original_dataset = datasets.ImageFolder(root=data_dir)
total_images = len(original_dataset.samples)
valid_images = [img_path for img_path, _ in original_dataset.samples if is_valid_image(img_path)]
valid_count = len(valid_images)

for class_name in original_dataset.classes:
    os.makedirs(os.path.join(clean_data_dir, class_name), exist_ok=True)

for img_path, label in original_dataset.samples:
    if img_path in valid_images:
        class_name = original_dataset.classes[label]
        shutil.copy(img_path, os.path.join(clean_data_dir, class_name))

print(f"Cleaned dataset created with {valid_count}/{total_images} valid images.")

# Model Training Function

def train_model(model, loss_function, optimizer, train_loader, test_loader, num_epochs=3):
    train_loss, test_loss, test_acc = [], [], []
    best_model_wts = model.state_dict()
    best_acc = 0.0

    early_stopping = EarlyStopping(patience=5)

    for epoch in range(num_epochs):
        print(f"Epoch {epoch + 1}/{num_epochs}")
        print("-" * 10)

        # Training phase
        model.train()
        running_loss = 0.0
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = loss_function(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * inputs.size(0)

        epoch_loss = running_loss / len(train_loader.dataset)
        train_loss.append(epoch_loss)

        # Evaluation phase
        model.eval()
        correct = 0
        total = 0
        test_running_loss = 0.0
        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = loss_function(outputs, labels)
                test_running_loss += loss.item() * inputs.size(0)

                _, preds = torch.max(outputs, 1)
                correct += (preds == labels).sum().item()
                total += labels.size(0)

        test_epoch_loss = test_running_loss / len(test_loader.dataset)
        test_epoch_acc = correct / total
        test_loss.append(test_epoch_loss)
        test_acc.append(test_epoch_acc)

        print(f"Train Loss: {epoch_loss:.4f}, Test Loss: {test_epoch_loss:.4f}, Test Acc: {test_epoch_acc:.4f}")

        # Saving the best model
        if test_epoch_acc > best_acc:
            best_acc = test_epoch_acc
            best_model_wts = model.state_dict()
            torch.save(best_model_wts, f"{model.__class__.__name__}_Best_Epoch.pth")
            print(f"Best model saved at epoch {epoch + 1} with accuracy: {best_acc:.4f}")

        # Checking Early Stopping condition
        early_stopping(test_epoch_acc)
        if early_stopping.early_stop:
            print("Early stopping triggered! Stopping training.")
            break  # Stop training if no improvement for patience epochs

    model.load_state_dict(best_model_wts)
    return model, train_loss, test_loss, test_acc

# Our Custom ResNet model
class CustomResNet50(nn.Module):
    def __init__(self, num_classes, dropout_rate):
        super(CustomResNet50, self).__init__()
        self.model = resnet50(weights=ResNet50_Weights.DEFAULT)
        self.model.fc = nn.Sequential(
            nn.Dropout(p=dropout_rate),
            nn.Linear(self.model.fc.in_features, num_classes)
        )

    def forward(self, x):
        return self.model(x)

# Finding Optimal Hyperparameters

learning_rates = [1e-3, 1e-4, 5e-5]
dropout_rates = [0.3, 0.4, 0.5]
batch_sizes = [16,32,64]

IMAGE_SIZE = (128, 128)
TEST_SPLIT = 0.2

# Best accuracy tracking
best_acc = 0.0
best_params = {}

first_loop = True
for lr in learning_rates:
    for batch_size in batch_sizes:
        for dropout in dropout_rates:
            print(f"\n Testing: LR={lr}, Batch Size={batch_size}, Dropout={dropout}")

            transform = transforms.Compose([
                transforms.Resize(IMAGE_SIZE),
                transforms.RandomHorizontalFlip(p=0.5),
                transforms.RandomRotation(degrees=15),
                transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.8, 1.0)),
                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
                transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),
                transforms.RandomGrayscale(p=0.1),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])

            full_dataset = datasets.ImageFolder(root=clean_data_dir, transform=transform)
            if(first_loop):
                show_images(full_dataset,5)
                first_loop = False
            print(f"Final dataset loaded from cleaned data: {len(full_dataset)} images.")
            test_size = int(TEST_SPLIT * len(full_dataset))
            train_size = len(full_dataset) - test_size
            train_dataset, test_dataset = random_split(full_dataset, [train_size, test_size])

            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

            # This is where we use the Transfer Learning :
            model = CustomResNet50(num_classes=len(full_dataset.classes), dropout_rate=dropout)

            model.to(torch.device("cuda" if torch.cuda.is_available() else "cpu"))
            optimizer = optim.Adam(model.parameters(), lr=lr)

            model, _, _, test_acc = train_model(model, nn.CrossEntropyLoss(), optimizer, train_loader, test_loader, num_epochs=10)

            if max(test_acc) > best_acc:
                best_acc = max(test_acc)
                best_params = {"learning_rate": lr, "batch_size": batch_size, "dropout": dropout}

# Making sure the loaders use the best Batch size (and not just the latest batch size)
train_loader = DataLoader(train_dataset, batch_size=best_params["batch_size"], shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=best_params["batch_size"], shuffle=False)

print(f"\n Best Hyperparameters: {best_params} with Accuracy: {best_acc:.4f}")

#  Baseline CNN Model
class BaselineCNN(nn.Module):
    def __init__(self, num_classes):
        super(BaselineCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.fc1 = nn.Linear(128 * 16 * 16, 512)
        self.fc2 = nn.Linear(512, num_classes)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.4)

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))
        x = self.pool(self.relu(self.conv3(x)))
        x = x.view(x.size(0), -1)
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

# Initialize Custom ResNet50 & Baseline Model

num_classes = len(full_dataset.classes)
baseline_model = BaselineCNN(num_classes)

# Using the best drop out with the resnet
best_dropout = best_params["dropout"]
resnet_model = CustomResNet50(num_classes=num_classes, dropout_rate=best_dropout)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
baseline_model.to(device)
resnet_model.to(device)

print(" Models ready: Baseline CNN & Custom ResNet50 with Dropout")

# Define Loss Function
loss_function = nn.CrossEntropyLoss()

# Optimizer for Baseline CNN
cnn_optimizer = optim.Adam(baseline_model.parameters(), lr=1e-4)

# Optimizer for Costum ResNet50
resnet_optimizer = optim.Adam(resnet_model.parameters(), lr=best_params["learning_rate"])

print(" Loss function & optimizers initialized.")

# Train Baseline CNN Model
print("\n Training Baseline CNN Model...")
baseline_model, cnn_train_loss, cnn_test_loss, cnn_test_acc = train_model(
    baseline_model, loss_function, cnn_optimizer, train_loader, test_loader, num_epochs=5
)
print("\n Finished Training Baseline CNN Model.")

# Train ResNet50 Model
print("\n Training ResNet50 Model...")
resnet_model, resnet_train_loss, resnet_test_loss, resnet_test_acc = train_model(
    resnet_model, loss_function, resnet_optimizer, train_loader, test_loader, num_epochs=10
)
print("\n Finished Training ResNet50 Model.")
print("\n Training Completed for both models!")

# Save Models

torch.save(baseline_model.state_dict(), "baseline_cnn_model.pth")
torch.save(resnet_model.state_dict(), "resnet50_model.pth")

print("\n Models saved successfully!")

# Model Evaluation

def evaluate_model(model, test_loader, model_name):
    """
    Evaluates the trained model and prints the classification report.
    NEW: Now evaluates both models and provides confusion matrix analysis.
    """
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    cm = confusion_matrix(all_labels, all_preds)

    # Plot confusion matrix
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=full_dataset.classes, yticklabels=full_dataset.classes)
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.title(f"Confusion Matrix - {model_name}")
    plt.show()

    # Print classification data
    print(f"\n Classification Data for {model_name}:")
    print(classification_report(all_labels, all_preds, target_names=full_dataset.classes))

# Evaluate both models
evaluate_model(baseline_model, test_loader, "Baseline CNN Model")
evaluate_model(resnet_model, test_loader, "Custom ResNet50 ")

# Results Visualization


def plot_results(train_loss, test_loss, test_acc, model_name):
    """
    Plots the training and testing loss, as well as the accuracy.
    """
    plt.figure(figsize=(12, 5))

    # Loss Plot
    plt.subplot(1, 2, 1)
    plt.plot(train_loss, label='Train Loss')
    plt.plot(test_loss, label='Test Loss')
    plt.title(f'{model_name} - Training and Testing Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    # Accuracy Plot
    plt.subplot(1, 2, 2)
    plt.plot(test_acc, label='Test Accuracy')
    plt.title(f'{model_name} - Testing Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    plt.show()

# Plot results for Baseline CNN
plot_results(cnn_train_loss, cnn_test_loss, cnn_test_acc, "Baseline CNN Model")

# Plot results for ResNet50
plot_results(resnet_train_loss, resnet_test_loss, resnet_test_acc, "Custom ResNet50")
